<?xml version="1.0" encoding="UTF-8"?> <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"> <channel> <title></title> <description>Debezium is an open source distributed platform for change data capture. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Debezium is durable and fast, so your apps can respond quickly and never miss an event, even when things go wrong.</description> <sy:updatePeriod>daily</sy:updatePeriod> <sy:updateFrequency>1</sy:updateFrequency> <link>https://debezium.io</link> <atom:link href="https://debezium.io/blog.atom" rel="self" type="application/rss+xml"/> <lastBuildDate>Sat, 04 Feb 2023 00:00:00 +0000</lastBuildDate> <item> <title>DDD Aggregates via CDC-CQRS Pipeline using Kafka &amp; Debezium</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;In this post, we are going to talk about a CDC-CQRS pipeline between a normalized relational database, MySQL, as the command database and a de-normalized NoSQL database, MongoDB, as the query database resulting in the creation of DDD Aggregates via Debezium &amp;amp; Kafka-Streams.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;You can find the complete source code of the example &lt;a href=&quot;https://github.com/purnima-jain/cdc-cqrs-pipeline&quot;&gt;here&lt;/a&gt;. Refer to the &lt;a href=&quot;https://github.com/purnima-jain/cdc-cqrs-pipeline/blob/master/README.md&quot;&gt;README.md&lt;/a&gt; for details on building and running the example code.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The example is centered around three microservices: &lt;code&gt;order-write-service&lt;/code&gt;, &lt;code&gt;order-aggregation-service&lt;/code&gt; and &lt;code&gt;order-read-service&lt;/code&gt;. These services are implemented as Spring-Boot applications in Java.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The &lt;code&gt;order-write-service&lt;/code&gt; exposes two REST endpoints which persist shipping-details and item-details in their respective tables on MySQL database. Debezium tails the MySQL bin logs to capture any events in both these tables and publishes messages to Kafka topics. These topics are consumed by &lt;code&gt;order-aggregation-service&lt;/code&gt; which is a Kafka-Streams application that joins data from both of these topics to create an Order-Aggregate object which is then published to a third topic. This topic is consumed by MongoDB Sink Connector and the data is persisted in MongoDB which is served by &lt;code&gt;order-read-service&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The overall architecture of the solution can be seen in the following diagram:&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt; &lt;div class=&quot;content&quot;&gt; &lt;img src=&quot;/assets/images/2023-02-04-ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/design_overview.png&quot; style=&quot;max-width:90%;&quot; class=&quot;responsive-image&quot;&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;sect1&quot;&gt; &lt;h2 id=&quot;rest_application_order_write_service&quot;&gt;REST Application: order-write-service&lt;/h2&gt; &lt;div class=&quot;sectionbody&quot;&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The first component that triggers the workflow starts is the &lt;code&gt;order-write-service&lt;/code&gt;. This has been implemented as a Spring-Boot application and exposes two REST end-points:&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;ulist&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;POST: &lt;code&gt;api/shipping-details&lt;/code&gt; to persist shipping details in the MySQL database&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;POST: &lt;code&gt;api/item-details&lt;/code&gt; to persist item details in the MySQL database&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Both of these endpoints persist their data in their respective tables in the MySQL database.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;sect1&quot;&gt; &lt;h2 id=&quot;command_database_mysql&quot;&gt;Command Database: MySQL&lt;/h2&gt; &lt;div class=&quot;sectionbody&quot;&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The backend processing of the above-mentioned REST endpoints culminates in persisting the data in their respective tables in MySQL.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Shipping details are stored in a table called &lt;code&gt;SHIPPING_DETAILS&lt;/code&gt;. And Item details are stored in a table called &lt;code&gt;ITEM_DETAILS&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Here is the data-model of &lt;code&gt;SHIPPING_DETAILS&lt;/code&gt; table, the column &lt;code&gt;ORDER_ID&lt;/code&gt; is its primary key:&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt; &lt;div class=&quot;content&quot;&gt; &lt;img src=&quot;/assets/images/2023-02-04-ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/shipping_details_data_model.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Here is the data-model of &lt;code&gt;ITEM_DETAILS&lt;/code&gt; table, the column &lt;code&gt;ORDER_ID&lt;/code&gt; + &lt;code&gt;ITEM_ID&lt;/code&gt; is its primary key:&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt; &lt;div class=&quot;content&quot;&gt; &lt;img src=&quot;/assets/images/2023-02-04-ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/item_details_data_model.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;sect1&quot;&gt; &lt;h2 id=&quot;kafka_connect_source_connector_mysql_cdc_debezium&quot;&gt;Kafka-Connect Source Connector: MySQL CDC Debezium&lt;/h2&gt; &lt;div class=&quot;sectionbody&quot;&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Change Data Capture (CDC) is a solution that captures change events from a database transaction log (called BinLogs in the case of MySQL) and forwards those events to downstream consumers ex. Kafka topic.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Debezium is a platform that provides a low latency data streaming platform for change data capture (CDC) and is built on top of Apache Kafka. It allows database row-level changes to be captured as events and published to Apache Kafka topics. We setup and configure Debezium to monitor our databases, and then our applications consume events for each row-level change made to the database.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;In our case, we will be using Debezium MySQL Source connector to capture any new events in the aforementioned tables and relay them to Apache Kafka. To achieve this, we will be registering our connecter by POST-ing the following JSON request to the REST API of Kafka Connect:&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;listingblock&quot;&gt; &lt;div class=&quot;content&quot;&gt; &lt;pre class=&quot;CodeRay highlight&quot;&gt;&lt;code data-lang=&quot;json&quot;&gt;{ &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;app-mysql-db-connector&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: { &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;connector.class&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;io.debezium.connector.mysql.MySqlConnector&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;tasks.max&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;database.hostname&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;mysql_db_server&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;database.port&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;3306&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;database.user&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;custom_mysql_user&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;database.password&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;custom_mysql_user_password&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;database.server.id&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;184054&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;database.server.name&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;app-mysql-server&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;database.whitelist&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;app-mysql-db&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;table.whitelist&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;app-mysql-db.shipping_details,app-mysql-db.item_details&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;database.history.kafka.bootstrap.servers&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;kafka_server:29092&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;database.history.kafka.topic&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;dbhistory.app-mysql-db&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;include.schema.changes&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;unwrap&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;transforms.unwrap.type&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;io.debezium.transforms.ExtractNewRecordState&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; } }&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;admonitionblock note&quot;&gt; &lt;table&gt; &lt;tr&gt; &lt;td class=&quot;icon&quot;&gt; &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt; &lt;/td&gt; &lt;td class=&quot;content&quot;&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The above configuration is based on Debezium 1.9.5.Final. Be aware that if you attempt to use the demo with Debezium 2.0+, a number of the above configuration properties have new names and the configuration will require some adjustments.&lt;/p&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;This sets up an instance of &lt;code&gt;io.debezium.connector.mysql.MySqlConnector&lt;/code&gt;, capturing changes from the specified MySQL instance. Note that by means of a table include list, only changes from the &lt;code&gt;SHIPPING_DETAILS&lt;/code&gt; and &lt;code&gt;ITEM_DETAILS&lt;/code&gt; tables are captured. It also applies a single message transform (SMT) named &lt;code&gt;ExtractNewRecordState&lt;/code&gt; which extracts the &lt;code&gt;after&lt;/code&gt; field from a Debezium change event in a Kafka record. The SMT replaces the original change event with only its &lt;code&gt;after&lt;/code&gt; field to create a simple Kafka record.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;By default, the Kafka topic name is “serverName.schemaName.tableName” which as per our connector configuration translates to:&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;ulist&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;app-mysql-server.app-mysql-db.item_details&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;app-mysql-server.app-mysql-db.shipping_details&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;sect1&quot;&gt; &lt;h2 id=&quot;kafka_streams_application_order_aggregation_service&quot;&gt;Kafka-Streams Application: order-aggregation-service&lt;/h2&gt; &lt;div class=&quot;sectionbody&quot;&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The Kafka-Streams application, namely &lt;code&gt;order-aggregation-service&lt;/code&gt;, is going to process data from the two Kafka cdc-topics. These topics receive CDC events based on the shipping-details and item-details relations found in MySQL.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;With that in place, the KStreams topology to create and maintain DDD order-aggregates on-the-fly can be built as follows.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The application reads the data from the shipping-details-cdc-topic. Since the Kafka topic records are in Debezium JSON format with unwrapped envelopes we need to parse the order-id and the shipping-details from it to create a KTable with order-id as the key and shipping-details as the value.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;listingblock&quot;&gt; &lt;div class=&quot;content&quot;&gt; &lt;pre class=&quot;CodeRay highlight&quot;&gt;&lt;code data-lang=&quot;java&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// Shipping Details Read&lt;/span&gt; KStream&amp;lt;&lt;span class=&quot;predefined-type&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;predefined-type&quot;&gt;String&lt;/span&gt;&amp;gt; shippingDetailsSourceInputKStream = streamsBuilder.stream(shippingDetailsTopicName, Consumed.with(STRING_SERDE, STRING_SERDE)); &lt;span class=&quot;comment&quot;&gt;// Change the Json value of the message to ShippingDetailsDto&lt;/span&gt; KStream&amp;lt;&lt;span class=&quot;predefined-type&quot;&gt;String&lt;/span&gt;, ShippingDetailsDto&amp;gt; shippingDetailsDtoWithKeyAsOrderIdKStream = shippingDetailsSourceInputKStream .map((orderIdJson, shippingDetailsJson) -&amp;gt; &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; KeyValue&amp;lt;&amp;gt;(parseOrderId(orderIdJson), parseShippingDetails(shippingDetailsJson))); &lt;span class=&quot;comment&quot;&gt;// Convert KStream to KTable&lt;/span&gt; KTable&amp;lt;&lt;span class=&quot;predefined-type&quot;&gt;String&lt;/span&gt;, ShippingDetailsDto&amp;gt; shippingDetailsDtoWithKeyAsOrderIdKTable = shippingDetailsDtoWithKeyAsOrderIdKStream.toTable( Materialized.&amp;lt;&lt;span class=&quot;predefined-type&quot;&gt;String&lt;/span&gt;, ShippingDetailsDto, KeyValueStore&amp;lt;Bytes, &lt;span class=&quot;type&quot;&gt;byte&lt;/span&gt;&lt;span class=&quot;type&quot;&gt;[]&lt;/span&gt;&amp;gt;&amp;gt;as(SHIPPING_DETAILS_DTO_STATE_STORE).withKeySerde(STRING_SERDE).withValueSerde(SHIPPING_DETAILS_DTO_SERDE));&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Similarly, the application reads the data from the item-details-cdc-topic and parses the order-id and the item from each individual message to group-by all the items pertaining to the same order-id in one list which is then aggregated to a KTable with order-id as key and the list of items pertaining to that specific order-id as value.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;listingblock&quot;&gt; &lt;div class=&quot;content&quot;&gt; &lt;pre class=&quot;CodeRay highlight&quot;&gt;&lt;code data-lang=&quot;java&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// Item Details Read&lt;/span&gt; KStream&amp;lt;&lt;span class=&quot;predefined-type&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;predefined-type&quot;&gt;String&lt;/span&gt;&amp;gt; itemDetailsSourceInputKStream = streamsBuilder.stream(itemDetailsTopicName, Consumed.with(STRING_SERDE, STRING_SERDE)); &lt;span class=&quot;comment&quot;&gt;// Change the Key of the message from ItemId + OrderId to only OrderId and parse the Json value to ItemDto&lt;/span&gt; KStream&amp;lt;&lt;span class=&quot;predefined-type&quot;&gt;String&lt;/span&gt;, ItemDto&amp;gt; itemDtoWithKeyAsOrderIdKStream = itemDetailsSourceInputKStream .map((itemIdOrderIdJson, itemDetailsJson) -&amp;gt; &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; KeyValue&amp;lt;&amp;gt;(parseOrderId(itemIdOrderIdJson), parseItemDetails(itemDetailsJson))); &lt;span class=&quot;comment&quot;&gt;// Group all the ItemDtos for each OrderId&lt;/span&gt; KGroupedStream&amp;lt;&lt;span class=&quot;predefined-type&quot;&gt;String&lt;/span&gt;, ItemDto&amp;gt; itemDtoWithKeyAsOrderIdKGroupedStream = itemDtoWithKeyAsOrderIdKStream.groupByKey(Grouped.with(STRING_SERDE, ITEM_DTO_SERDE)); &lt;span class=&quot;comment&quot;&gt;// Aggregate all the ItemDtos pertaining to each OrderId in a list&lt;/span&gt; KTable&amp;lt;&lt;span class=&quot;predefined-type&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;predefined-type&quot;&gt;ArrayList&lt;/span&gt;&amp;lt;ItemDto&amp;gt;&amp;gt; itemDtoListWithKeyAsOrderIdKTable = itemDtoWithKeyAsOrderIdKGroupedStream.aggregate( (Initializer&amp;lt;&lt;span class=&quot;predefined-type&quot;&gt;ArrayList&lt;/span&gt;&amp;lt;ItemDto&amp;gt;&amp;gt;) &lt;span class=&quot;predefined-type&quot;&gt;ArrayList&lt;/span&gt;::&lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt;, (orderId, itemDto, itemDtoList) -&amp;gt; addItemToList(itemDtoList, itemDto), Materialized.&amp;lt;&lt;span class=&quot;predefined-type&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;predefined-type&quot;&gt;ArrayList&lt;/span&gt;&amp;lt;ItemDto&amp;gt;, KeyValueStore&amp;lt;Bytes, &lt;span class=&quot;type&quot;&gt;byte&lt;/span&gt;&lt;span class=&quot;type&quot;&gt;[]&lt;/span&gt;&amp;gt;&amp;gt;as(ITEM_DTO_STATE_STORE).withKeySerde(STRING_SERDE).withValueSerde(ITEM_DTO_ARRAYLIST_SERDE));&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;With both the KTables having order-id as the key, it’s easy enough to join them using order-id to create an aggregate called Order-Aggregate. Order-Aggregate is a composite object created by assimilating data from both the shipping-details as well as the item-details. This Order-Aggregate is then written to an order-aggregate Kafka topic.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;listingblock&quot;&gt; &lt;div class=&quot;content&quot;&gt; &lt;pre class=&quot;CodeRay highlight&quot;&gt;&lt;code data-lang=&quot;java&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// Joining the two tables: shippingDetailsDtoWithKeyAsOrderIdKTable and itemDtoListWithKeyAsOrderIdKTable&lt;/span&gt; ValueJoiner&amp;lt;ShippingDetailsDto, &lt;span class=&quot;predefined-type&quot;&gt;ArrayList&lt;/span&gt;&amp;lt;ItemDto&amp;gt;, OrderAggregate&amp;gt; shippingDetailsAndItemListJoiner = (shippingDetailsDto, itemDtoList) -&amp;gt; instantiateOrderAggregate(shippingDetailsDto, itemDtoList); KTable&amp;lt;&lt;span class=&quot;predefined-type&quot;&gt;String&lt;/span&gt;, OrderAggregate&amp;gt; orderAggregateKTable = shippingDetailsDtoWithKeyAsOrderIdKTable.join(itemDtoListWithKeyAsOrderIdKTable, shippingDetailsAndItemListJoiner); &lt;span class=&quot;comment&quot;&gt;// Outputting to Kafka Topic&lt;/span&gt; orderAggregateKTable.toStream().to(orderAggregateTopicName, Produced.with(STRING_SERDE, ORDER_AGGREGATE_SERDE));&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;sect1&quot;&gt; &lt;h2 id=&quot;kafka_connect_sink_connector_mongodb_connector&quot;&gt;Kafka-Connect Sink Connector: MongoDB Connector&lt;/h2&gt; &lt;div class=&quot;sectionbody&quot;&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The sink connector is a Kafka Connect connector that reads data from Apache Kafka and writes data to some data-store. Using a MongoDB sink connector, it is easy to have the DDD aggregates written into MongoDB. All it needs is a configuration which can be posted to the REST API of Kafka Connect in order to run the connector.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;listingblock&quot;&gt; &lt;div class=&quot;content&quot;&gt; &lt;pre class=&quot;CodeRay highlight&quot;&gt;&lt;code data-lang=&quot;json&quot;&gt;{ &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;app-mongo-sink-connector&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: { &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;connector.class&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;com.mongodb.kafka.connect.MongoSinkConnector&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;topics&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;order_aggregate&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;connection.uri&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;mongodb://root_mongo_user:root_mongo_user_password@mongodb_server:27017&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;key.converter&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;org.apache.kafka.connect.storage.StringConverter&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;value.converter&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;org.apache.kafka.connect.json.JsonConverter&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;value.converter.schemas.enable&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;value&quot;&gt;false&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;order_db&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;collection&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;document.id.strategy.overwrite.existing&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;document.id.strategy&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInKeyStrategy&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;hk,hv&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;transforms.hk.type&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;org.apache.kafka.connect.transforms.HoistField$Key&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;transforms.hk.field&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;_id&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;transforms.hv.type&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;org.apache.kafka.connect.transforms.HoistField$Value&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;transforms.hv.field&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; } }&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;sect1&quot;&gt; &lt;h2 id=&quot;query_database_mongodb&quot;&gt;Query Database: MongoDB&lt;/h2&gt; &lt;div class=&quot;sectionbody&quot;&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The DDD aggregate is written to the database &lt;code&gt;order_db&lt;/code&gt; in the collection &lt;code&gt;order&lt;/code&gt; on MongoDB. The order-id becomes the &lt;code&gt;_id&lt;/code&gt; of the table and the &lt;code&gt;order&lt;/code&gt; column stores the order-aggregate as JSON.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;sect1&quot;&gt; &lt;h2 id=&quot;rest_application_order_read_service&quot;&gt;REST Application: order-read-service&lt;/h2&gt; &lt;div class=&quot;sectionbody&quot;&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The Order Aggregate persisted in MongoDB is served via a REST endpoint in &lt;code&gt;order-read-service&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;ulist&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;GET: &lt;code&gt;api/order/{order-id}&lt;/code&gt; to retrieve the order from the MongoDB database&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;sect1&quot;&gt; &lt;h2 id=&quot;execution_instructions&quot;&gt;Execution Instructions&lt;/h2&gt; &lt;div class=&quot;sectionbody&quot;&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The complete source code for this blog post is provided &lt;a href=&quot;https://github.com/purnima-jain/cdc-cqrs-pipeline&quot;&gt;here&lt;/a&gt; in Github. Begin by cloning this repository and changing into the &lt;code&gt;cdc-cqrs-pipeline&lt;/code&gt; directory. The project provides a Docker Compose file with services for all the components:&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;ulist&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;MySQL&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Adminer (formerly known as phpMinAdmin), to manage MySQL via browser&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;MongoDB&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Mongo Express, to manage MongoDB via browser&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Zookeeper&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Confluent Kafka&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Kafka Connect&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Once all services have started, register an instance of the Debezium MySQL connector &amp;amp; MongoDB Connector by executing the &lt;code&gt;Create-MySQL-Debezium-Connector&lt;/code&gt; and &lt;code&gt;Create-MongoDB-Sink-Connector&lt;/code&gt; request respectively from &lt;code&gt;cdc-cqrs-pipeline.postman_collection.json&lt;/code&gt;. Execute the request &lt;code&gt;Get-All-Connectors&lt;/code&gt; to verify that the connectors have been properly created.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Change into the individual directories and spin-up the three Spring-Boot applications:&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;ulist&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;order-write-service&lt;/code&gt;: runs on port no &lt;code&gt;8070&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;order-aggregation-service&lt;/code&gt;: runs on port no &lt;code&gt;8071&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;order-read-service&lt;/code&gt;: runs on port no &lt;code&gt;8072&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;With this, our setup is complete.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;To test the application, execute the request &lt;code&gt;Post-Shipping-Details&lt;/code&gt; from the postman collection to insert shipping-details and &lt;code&gt;Post-Item-Details&lt;/code&gt; to insert item-details for a particular order id.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Finally, execute the &lt;code&gt;Get-Order-By-Order-Id&lt;/code&gt; request in the postman collection to retrieve the complete Order Aggregate.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;sect1&quot;&gt; &lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt; &lt;div class=&quot;sectionbody&quot;&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Apache Kafka acts as a highly scalable and reliable backbone for the messaging amongst the services. Putting Apache Kafka into the center of the overall architecture also ensures a decoupling of involved services. If for instance single components of the solution fail or are not available for some time, events will simply be processed later on: after a restart, the Debezium connector will continue to tail the relevant tables from the point where it left off before. Similarly, any consumer will continue to process topics from its previous offset. By keeping track of already successfully processed messages, duplicates can be detected and excluded from repeated handling.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Naturally, such event pipeline between different services is eventually consistent, i.e. consumers such as the order-read-service may lag a bit behind producers such as the order-write-service. Usually, that’s just fine, though, and can be handled in terms of the application’s business logic. Also, end-to-end delays of the overall solution are typically low (seconds or even sub-second range), thanks to log-based change data capture which allows for emission of events in near-realtime.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;</description> <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2023/02/04/ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/</link> <guid isPermaLink="true">https://debezium.io/blog/2023/02/04/ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/</guid> </item> <item> <title>Debezium 2.1.2.Final Released</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The Debezium release cadence is in full swing as I&amp;#8217;m excited to announce Debezium &lt;strong&gt;2.1.2.Final&lt;/strong&gt;!&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;This release focuses primarily on bug fixes and stability; and it is the recommended update for all users from earlier versions. This release contains &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project+%3D+DBZ+AND+fixVersion+%3D+2.1.2.Final&quot;&gt;28 resolved issues&lt;/a&gt;, so let&amp;#8217;s take a moment and discuss a critical breaking change.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Thu, 26 Jan 2023 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2023/01/26/debezium-2-1-2-final-released/</link> <guid isPermaLink="true">https://debezium.io/blog/2023/01/26/debezium-2-1-2-final-released/</guid> </item> <item> <title>We Are Hiring (Saga continues)</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;In November last year, we &lt;a href=&quot;/blog/2022/11/15/filling-the-ranks/&quot;&gt;announced&lt;/a&gt; we were looking for reinforcements for the team. And I have two pieces of news for you today: a good one and an even better one.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Tue, 24 Jan 2023 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2023/01/24/we-are-hiring-2/</link> <guid isPermaLink="true">https://debezium.io/blog/2023/01/24/we-are-hiring-2/</guid> </item> <item> <title>Debezium 2.2.0.Alpha1 Released</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;It&amp;#8217;s my pleasure to announce not only the first release of the Debezium 2.2 series, but also the first release of Debezium in 2023, &lt;strong&gt;2.2.0.Alpha&lt;/strong&gt;!&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The Debezium 2.2.0.Alpha1 release includes some breaking changes, a number of bug fixes, and some noteworthy improvements and features, including but not limited to:&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;ulist&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;[Breaking Change] - &lt;code&gt;ZonedTimestamp&lt;/code&gt; values will no longer truncate fractional seconds.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;[New] - Support ingesting changes from an Oracle logical stand-by database&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;[New] - Support Amazon S3 buckets using the Debezium Storage API&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;[New] - Support retrying database connections during connector start-up&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;[New] - Debezium Server sink connector support for Apache RocketMQ and Infinispan&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;</description> <pubDate>Thu, 19 Jan 2023 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2023/01/19/debezium-2-2-alpha1-released/</link> <guid isPermaLink="true">https://debezium.io/blog/2023/01/19/debezium-2-2-alpha1-released/</guid> </item> <item> <title>Change Data Capture with QuestDB and Debezium</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;This tutorial was originally published by &lt;a href=&quot;https://questdb.io/&quot;&gt;QuestDB&lt;/a&gt;, where guest contributor, &lt;a href=&quot;https://yitaek.medium.com/&quot;&gt;Yitaek Hwang&lt;/a&gt;, shows us how to stream data into QuestDB with change data capture via Debezium and Kafka Connect.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Fri, 06 Jan 2023 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2023/01/06/change-data-capture-with-questdb-and-debezium/</link> <guid isPermaLink="true">https://debezium.io/blog/2023/01/06/change-data-capture-with-questdb-and-debezium/</guid> </item> <item> <title>Debezium 2.1.0.Final/Debezium 2.1.1.Final Released</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Today it&amp;#8217;s my great pleasure to announce the availability of Debezium &lt;strong&gt;2.1.0.Final&lt;/strong&gt;!&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;You might recently noticed that Debezium went a bit silent for the last few weeks. No, we are not going away. In fact the elves in Google worked furiously to bring you a present under a Christmas tree - Debezium Spanner connector.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Thu, 22 Dec 2022 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/12/22/debezium-2-1-final-released/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/12/22/debezium-2-1-final-released/</guid> </item> <item> <title>Filling the Ranks</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;As you are probably well aware, Gunnar Morling has stepped down from his position as Debezium project lead and is now pursuing new exciting adventures. It is sad, but every cloud has a silver lining!&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;What can it be? We (the Debezium team and Red Hat) are hiring! Are you a community contributor? Do you have any pull requests under your belt? Are you a happy Debezium user and eager to do more, or are you a seasoned Java developer looking for work in an exciting and inclusive open-source environment?&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Tue, 15 Nov 2022 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/11/15/filling-the-ranks/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/11/15/filling-the-ranks/</guid> </item> <item> <title>Debezium 2.1.0.Alpha1 Released</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;It&amp;#8217;s my pleasure to announce the first release of the Debezium 2.1 series, &lt;strong&gt;2.1.0.Alpha1&lt;/strong&gt;!&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The Debezium 2.1.0.Alpha1 release includes quite a number of bug fixes but also some noteworthy improvements and new features including but not limited to:&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;ulist&quot;&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Support for PostgreSQL 15&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Single Message Transformation (SMT) predicate support in Debezium engine&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Capturing TRUNCATE as change event in MySQL table topics&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Oracle LogMiner performance improvements&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;New Redis-based storage module&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;</description> <pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/11/10/debezium-2-1-alpha1-released/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/11/10/debezium-2-1-alpha1-released/</guid> </item> <item> <title>Debezium Evolving</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Some time in early 2017, I got a meeting invite from Debezium&amp;#8217;s founder, &lt;a href=&quot;https://twitter.com/rhauch&quot;&gt;Randall Hauch&lt;/a&gt;. He was about to begin a new chapter in his professional career and was looking for someone to take over as the project lead for Debezium. So we hopped on a call to talk things through, and I was immediately sold on the concept of change data capture, its large number of potential use cases and applications, and the idea of making this available to the community as open-source. After some short consideration I decided to take up this opportunity, and without a doubt this has been one of the best decisions I&amp;#8217;ve ever made in my job.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/10/26/debezium-evolving/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/10/26/debezium-evolving/</guid> </item> <item> <title>Debezium 1.9.7.Final Released</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;I&amp;#8217;m excited to announce the release of Debezium &lt;strong&gt;1.9.7.Final&lt;/strong&gt;!&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;This release focuses on bug fixes and stability; and is the recommended update for all users from earlier versions. This release contains &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project+%3D+DBZ+AND+fixVersion+%3D+1.9.7.Final&quot;&gt;22 resolved issues&lt;/a&gt; overall.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/10/26/debezium-1-9-7-final-released/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/10/26/debezium-1-9-7-final-released/</guid> </item> <item> <title>Debugging flaky tests</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;When developing the tests for your project, sooner or later you will probably get into the situation when some of the tests fail randomly. These tests, also known as flaky tests, are very unpleasant as you never know if the failure was random or there is a regression in your code. In the worst case you just ignore these tests because you know they are flaky. Most of the testing frameworks even have a dedicated annotation or other means to express that the test is flaky and if it fails, the failure should be ignored. The value of such a test is very questionable. The best thing you can do with such a test is of course to fix it so that it doesn&amp;#8217;t fail randomly. That&amp;#8217;s easy to say, but harder to do. The hardest part is usually to make the test fail in your development environment so that you can debug it and understand why it fails and what is the root cause of the failure. In this blog post I&amp;#8217;ll try to show a few techniques which may help you to simulate random test failures on you local machine.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Thu, 20 Oct 2022 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/10/20/flaky-tests/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/10/20/flaky-tests/</guid> </item> <item> <title>Debezium 2.0.0.Final Released</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Today it&amp;#8217;s my great pleasure to announce the availability of Debezium &lt;strong&gt;2.0.0.Final&lt;/strong&gt;!&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Since our 1.0 release in December 2019, the community has worked vigorously to build a comprehensive open-source low-latency platform for change data capture (CDC). Over the past three years, we have extended Debezium&amp;#8217;s portfolio to include a stable connector for Oracle, a community led connector for Vitess, the introduction of incremental snapshots, multi-partition support, and so much more. With the help of our active community of contributors and committers, Debezium is the de facto leader in the CDC space, deployed to production within lots of organizations from across multiple industries, using hundreds of connectors to stream data changes out of thousands of database platforms.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;The 2.0 release marks a new milestone for Debezium, one that we are proud to share with each of you.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Mon, 17 Oct 2022 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/10/17/debezium-2-0-final-released/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/10/17/debezium-2-0-final-released/</guid> </item> <item> <title>Debezium 2.0.0.CR1 Released</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;I am excited to announce the release of Debezium &lt;strong&gt;2.0.0.CR1&lt;/strong&gt;!&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;This release contains breaking changes, stability fixes, and bug fixes, all to inch us closer to 2.0.0.Final. Overall, this release contains a total of &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.0.0.CR1%20ORDER%20BY%20component%20ASC&quot;&gt;53 issues&lt;/a&gt; that were fixed.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Mon, 10 Oct 2022 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/10/10/debezium-2.0-cr1-released/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/10/10/debezium-2.0-cr1-released/</guid> </item> <item> <title>Debezium for Oracle - Part 2: Running the connector</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;This post is part of a 3-part series to explore using Debezium to ingest changes from an Oracle database using Oracle LogMiner. In case you missed it, the first part of this series is &lt;a href=&quot;/blog/2022/09/30/debezium-oracle-series-part-1/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;In this second installment, we will build on what we did in part one by deploying the Oracle connector using Zookeeper, Kafka, and Kafka Connect. We are going to discuss a variety of configuration options for the connector and why they&amp;#8217;re essential. And finally, we&amp;#8217;re going to see the connector in action!&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Thu, 06 Oct 2022 12:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/10/06/debezium-oracle-series-part-2/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/10/06/debezium-oracle-series-part-2/</guid> </item> <item> <title>Debezium for Oracle - Part 1: Installation and Setup</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;This post is part of a 3-part series to explore using Debezium to ingest changes from an Oracle database using Oracle LogMiner. Throughout the series, we&amp;#8217;ll examine all the steps to setting up a proof of concept (POC) deployment for Debezium for Oracle. We will discuss setup and configurations as well as the nuances of multi-tenancy. We will also dive into any known pitfalls and concerns you may need to know and how to debug specific problems. And finally, we&amp;#8217;ll talk about performance and monitoring to maintain a healthy connector deployment.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Throughout this exercise, we hope that this will show you just how simple it is to deploy Debezium for Oracle. This installation and setup portion of the series may seem quite complicated, but many of these steps likely already exist in a pre-existing environment. We will dive into each step, explaining it is essential should you use a container image deployment.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Fri, 30 Sep 2022 12:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/09/30/debezium-oracle-series-part-1/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/09/30/debezium-oracle-series-part-1/</guid> </item> <item> <title>Debezium 1.9.6.Final Released</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;I&amp;#8217;m excited to announce the release of Debezium &lt;strong&gt;1.9.6.Final&lt;/strong&gt;!&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;This release focuses on bug fixes and stability; and is the recommended update for all users from earlier versions. This release contains &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project+%3D+DBZ+AND+fixVersion+%3D+1.9.6.Final&quot;&gt;78 resolved issues&lt;/a&gt; overall.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Mon, 26 Sep 2022 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/09/26/debezium-1-9-6-final-released/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/09/26/debezium-1-9-6-final-released/</guid> </item> <item> <title>Debezium 2.0.0.Beta2 Released</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;I am excited to announce the release of Debezium &lt;strong&gt;2.0.0.Beta2&lt;/strong&gt;!&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;This release contains several breaking changes, stability fixes, and bug fixes, all to inch us closer to 2.0.0.Final. Overall, this release contains a total of &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.0.0.Beta2%20ORDER%20BY%20component%20ASC&quot;&gt;107 issues&lt;/a&gt; that were fixed.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Fri, 16 Sep 2022 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/09/16/debezium-2.0-beta2-released/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/09/16/debezium-2.0-beta2-released/</guid> </item> <item> <title>Debezium 2.0.0.Beta1 Released</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;I am thrilled to share that Debezium &lt;strong&gt;2.0.0.Beta1&lt;/strong&gt; has been released!&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;This release contains several new features including a pluggable topic selector, the inclusion of database user who committed changes for Oracle change events, and improved handling of table unique indices as primary keys. In addition, there are several breaking changes such as the move to multi-partition mode as default and the introduction of the &lt;code&gt;debezium-storage&lt;/code&gt; module and its implementations. So lets take a look at all these in closer detail.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/07/27/debezium-2.0-beta1-released/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/07/27/debezium-2.0-beta1-released/</guid> </item> <item> <title>Debezium 1.9.5.Final Released</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;With the summer in full swing, the team is pleased to announce the release of Debezium &lt;strong&gt;1.9.5.Final&lt;/strong&gt;!&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;This release primarily focuses on bugfixes and stability; and is the recommended update for all users from earlier versions. This release contains &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project+%3D+DBZ+AND+fixVersion+%3D+1.9.5.Final&quot;&gt;24 resolved issues&lt;/a&gt; overall.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/07/11/debezium-1-9-5-final-released/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/07/11/debezium-1-9-5-final-released/</guid> </item> <item> <title>Debezium 2.0.0.Alpha3 Released</title> <description>&lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;I am thrilled to share that Debezium &lt;strong&gt;2.0.0.Alpha3&lt;/strong&gt; has been released!&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;While this release contains a plethora of bugfixes, there are a few noteworthy improvements, which include providing a timestamp in transaction metadata events, the addition of several new fields in Oracle&amp;#8217;s change event source block, and a non-backward compatible change to the Oracle connector&amp;#8217;s offsets.&lt;/p&gt; &lt;/div&gt; &lt;div class=&quot;paragraph&quot;&gt; &lt;p&gt;Lets take a look at these in closer detail.&lt;/p&gt; &lt;/div&gt;</description> <pubDate>Tue, 05 Jul 2022 00:00:00 +0000</pubDate> <link>https://debezium.io/blog/2022/07/05/debezium-2.0-alpha3-released/</link> <guid isPermaLink="true">https://debezium.io/blog/2022/07/05/debezium-2.0-alpha3-released/</guid> </item> </channel> </rss>